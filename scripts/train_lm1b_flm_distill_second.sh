python -u -m main \
    loader.global_batch_size=512 \
    loader.batch_size=128 \
    loader.eval_batch_size=128 \
    data=lm1b-wrap \
    data.cache_dir=/home/work/RADAR/workspace/KAIST/cdBDD/datasets/lm1b \
    wandb.project=flm_distill \
    wandb.name=second_phase_distill \
    model=small \
    algo=flm_distill_double \
    algo.teacher_f_path=/home/work/RADAR/workspace/KAIST/discrete-mean-flow/text/outputs/lm1b/2026.01.07/000819/checkpoints/73-1000000.ckpt \
    algo.teacher_g_path="/home/work/RADAR/workspace/KAIST/discrete-mean-flow/text/outputs/lm1b/2026.01.22/015052/checkpoints/last.ckpt" \
    trainer.max_steps=1000000 \
    trainer.precision=bf16 \
    trainer.val_check_interval=5000 \
    model.length=128 \
    sampling.steps=[1,2,4,32] \
    optim.lr=3e-4 \
    algo.double_temb=True \
    algo.add_boundary=True \
    +algo.boundary_prob=128 \
    algo.learnable_loss_weighting=True \
    callbacks.checkpoint_every_n_steps.every_n_train_steps=20000 \
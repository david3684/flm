DIT_USE_COMPILE=False python -u -m main \
    loader.global_batch_size=128 \
    loader.batch_size=16 \
    loader.eval_batch_size=16 \
    data=openwebtext-split \
    data.cache_dir=YOUR_DATASET_PATH \
    wandb.project=owt_full \
    wandb.name=psd_lr_3e-4_weight_alpha_batch_128_zero_init_adam_progressive_10k_add_t0 \
    model=small \
    algo=flm_shortcut_distill \
    training.loss_type=shortcut \
    +algo.teacher_path="YOUR_TEACHER_CHECKPOINT_PATH" \
    trainer.max_steps=1000000 \
    trainer.precision=bf16 \
    trainer.val_check_interval=2500 \
    model.length=1024 \
    sampling.steps=[1,2,4,8,16,32,64,128,256,512] \
    "+sampling.noise_removal_list=[shortcut_alpha]" \
    sampling.solver=euler \
    optim.lr=3e-4 \
    algo.shortcut_loss_type=mse \
    algo.double_temb=True \
    algo.use_discrete_schedule=True \
    algo.sample_d_on_grid=True \
    algo.use_continuous_shortcut=False \
    algo.shortcut_k_max=1024 \
    algo.shortcut_on_alpha_t=True \
    +algo.sample_midpoint=False \
    +algo.base_num_steps=512 \
    algo.add_boundary=True \
    +algo.boundary_prob=32 \
    algo.bootstrap_argmax=False \
    algo.n_separated_blocks=-1 \
    algo.shortcut_scale_loss=False \
    algo.zero_center_residual=False \
    algo.learnable_loss_weighting=True \
    +algo.freeze_token_embeddings=False \
    callbacks.checkpoint_every_n_steps.every_n_train_steps=10000 \
    optim.use_muon=False \
    +algo.progressive_distill=True \
    +algo.final_layer_remove_bias=False \
    +algo.final_layer_weight_decay=False \
    trainer.limit_val_batches=10 \
    # checkpointing.resume_from_ckpt=True \
    
